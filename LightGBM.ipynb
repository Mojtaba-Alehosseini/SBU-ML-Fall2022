{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWLITIyHjF5dfqzaJao11E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mojtaba-Alehosseini/SBU-ML-Fall2022/blob/main/LightGBM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mojtaba Alehosseini , 401422018 , Feb2023"
      ],
      "metadata": {
        "id": "myh0SXaXLWI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# LightGBM: A highly efficient gradient boosting decision tree\n",
        "\n",
        "- Authors : Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu\n",
        "- Journal : Advances in neural information processing systems\n",
        "- Publication date : 2017\n",
        "- Link : [LightGBM](https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf)\n",
        "- Cited by 6580\n",
        "\n",
        "## Abstract\n",
        "\n",
        "> **Ensemble learning** is a machine learning technique that combines the predictions of multiple models to produce a more accurate and robust prediction. It leverages the idea that multiple models can learn complementary information from the data and therefore can produce a more robust prediction than any single model. The combination of predictions from multiple models can be done by either voting (for classification problems) or by averaging (for regression problems). Some popular ensemble methods include bagging, boosting, random forests, and stacking.\n",
        "\n",
        "> **LightGBM** is a machine learning algorithm that uses decision trees to make predictions. It is fast and efficient, especially for **large datasets**, and is used for both regression (predicting a continuous value) and classification (predicting a label) tasks. It works by building multiple trees and combining their predictions to make the final result. This approach is called \"boosting\" and helps improve the accuracy of the model. \n",
        "\n",
        "> LightGBM is a highly optimized gradient boosting framework that utilizes tree-based learning algorithms to solve a wide range of machine learning problems. One of the key innovations in LightGBM is its use of a novel gradient-based one-side sampling (**GOSS**) algorithm, which makes it possible to handle large datasets with a limited memory budget. The algorithm works by selecting a small subset of data to use as the basis for each tree, rather than using the entire dataset, and updating the weights of each tree based on the gradient of the loss function. Another one is Exclusive Feature Bundling (**EFB**) algorithm, It involves grouping features into exclusive bundles and training separate trees for each bundle instead of considering all features in a single tree, by bundling features in this way, LightGBM is able to capture non-linear relationships between features that may not be easily detected by traditional gradient boosting methods. It also allows for improved interpretability, as it can be easier to understand the relationship between a smaller set of features compared to a large number of features.\n",
        "\n",
        "> - GBDTs (Gradient Boosting Decision Tree) need to scan all the data instances to estimate the information gain of all possible split points, which is very **time** consuming. Their computational complexities will be proportional to both the number of features and the number of instances.\n",
        "\n",
        ">> âž” Gradient-based One-Side Sampling (**GOSS**) and Exclusive Feature Bundling (**EFB**).\n",
        ">> 1. With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain.\n",
        ">> 2. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features.\n",
        "\n",
        "## Principal Method\n",
        "- **Gradient Boosting Decision Tree (GBDT)**\n",
        "- Authors : Jerome H Friedman\n",
        "- Journal : Annals of statistics\n",
        "- Publication date : 2001\n",
        "- Link : [GBDT](https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.pdf)\n",
        "- Cited by 20586\n",
        "\n",
        "### Theory:\n",
        ">Gradient Boosting Decision Tree (GBDT) is an ensemble learning method that combines multiple decision trees to improve the accuracy of the predictions. It is based on the boosting algorithm and works by training multiple decision trees in a sequential manner, where each tree tries to correct the mistakes made by the previous trees. The final prediction is made by combining the predictions of all the trees through a weighted average.\n",
        "\n",
        "### Mathematics\n",
        "\n",
        "> Like other boosting methods, gradient boosting combines weak \"learners\" into a strong learner in an iterative fashion. It is easiest to explain in the least-squares regression setting, where the goal is to predict values $\\hat{y} = F(x)$ by minimizing the mean squared error $\\frac{1}{n} \\sum_{i} (\\hat{y}_{i} - y_{i})^{2}$,\n",
        ">\n",
        "> where $i$ indexes over the training set of size $n$ and $y$ is the actual value of the output variable.\n",
        "\n",
        "> At each stage $m (1 \\le m \\le M)$ of gradient boosting, an imperfect model $F_{m}$ is improved by adding a new estimator $h_{m}(x)$. This results in $F_{m+1}(x_{i}) = F_{m}(x_{i}) + h_{m}(x_{i}) = y_{i}$, or equivalently, $h_{m}(x_{i}) = y_{i} - F_{m}(x_{i})$. The algorithm fits $h_{m}$ to the residual $y_{i} - F_{m}(x_{i})$.\n",
        "\n",
        "> Gradient boosting is a gradient descent algorithm, where the loss and its gradient are \"plugged in\". For MSE loss, $L_{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{i} - F(x_{i}))^{2}$, its negative gradient is proportional to the residual $-\\frac {\\partial L_{\\rm {MSE}}}{\\partial F(x_{i})} = \\frac{2}{n} (y_{i} - F(x_{i})) = \\frac{2}{n} h_{m}(x_{i})$.\n",
        "\n",
        "> In many supervised learning problems there is an output variable $y$ and a vector of input variables $x$, related to each other with some probabilistic distribution. The goal is to find some function $\\hat{F}(x)$ that best approximates the output variable from the values of input variables. This is formalized by introducing some loss function $L(y, F(x))$ and minimizing it in expectation:\n",
        ">\n",
        "> $\\hat{F}=\\arg\\min_{F}\\mathbb{E}_{x,y}[L(y,F(x))]$\n",
        "\n",
        "> The gradient boosting method assumes a real-valued $y$. It seeks an approximation $\\hat{F}(x)$ in the form of a weighted sum of $M$ functions $h_{m}(x)$ from some class $\\mathcal{H}$, called base (or weak) learners:\n",
        ">\n",
        "> $\\hat{F}(x)=\\sum_{m=1}^{M}\\gamma_{m}h_{m}(x)+{\\mbox{const}}$\n",
        "\n",
        "> We are usually given a training set $\\{(x_{1},y_{1}),\\dots ,(x_{n},y_{n})\\}$ of known sample values of $x$ and corresponding values of $y$. In accordance with the empirical risk minimization principle, the method tries to find an approximation $\\hat{F}(x)$ that minimizes the average value of the loss function on the training set, i.e., minimizes the empirical risk. It does so by starting with a model, consisting of a constant function $F_{0}(x)$, and incrementally expands it in a greedy fashion:\n",
        ">\n",
        "> $F_{0}(x)=\\arg\\min_{\\gamma}{\\sum_{i=1}^{n}{L(y_{i},\\gamma)}}$\n",
        ">\n",
        "> $F_{m}(x)=F_{m-1}(x)+\\arg\\min_{h_{m}\\in {\\mathcal {H}}}\\left[{\\sum _{i=1}^{n}{L(y_{i},F_{m-1}(x_{i})+h_{m}(x_{i}))}}\\right]$ for $m\\geq 1$, where $h_{m}\\in {\\mathcal {H}}$ is a base learner function.\n",
        "\n",
        "> Unfortunately, choosing the best function $h_{m}$ at each step for an arbitrary loss function $L$ is a computationally infeasible optimization problem in general. Therefore, we restrict our approach to a simplified version of the problem.\n",
        ">\n",
        "> The idea is to apply a steepest descent step to this minimization problem (functional gradient descent).\n",
        ">\n",
        "> The basic idea behind the steepest descent is to find a local minimum of the loss function by iterating on $F_{m-1}(x)$. In fact, the local maximum-descent direction of the loss function is the negative gradient.\n",
        ">\n",
        "> Hence, moving a small amount $\\gamma$ such that the linear approximation remains valid:\n",
        ">\n",
        "> ${\\displaystyle F_{m}(x)=F_{m-1}(x)-\\gamma \\sum _{i=1}^{n}{\\nabla _{F_{m-1}}L(y_{i},F_{m-1}(x_{i}))}}$\n",
        ">\n",
        "> where $\\gamma >0$ For small $\\gamma$ , this implies that ${\\displaystyle L(y_{i},F_{m}(x_{i}))\\leq L(y_{i},F_{m-1}(x_{i}))}$\n",
        "\n",
        "### Algorithm:\n",
        "\n",
        "> Input: training set $\\{(x_{i},y_{i})\\}_{i=1}^{n}$, a differentiable loss function $L(y,F(x))$ number of iterations $M$.\n",
        ">\n",
        "> Algorithm:\n",
        ">\n",
        "> > 1. Initialize model with a constant value:\n",
        "> > >\n",
        "> > > $ F_0(x) = \\underset{\\gamma}{\\arg\\min} \\sum_{i=1}^n L(y_i, \\gamma)$.\n",
        "> >\n",
        "> > 2. For $m = 1$ to $M$:\n",
        "> > >\n",
        "> > > a. Compute pseudo-residuals:\n",
        "> > >\n",
        "> > > $r_{im} = -\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x)=F_{m-1}(x)} \\quad \\mbox{for } i=1,\\ldots,n$.\n",
        "> > >\n",
        "> > > b. Fit a base learner (or weak learner, e.g. tree) closed under scaling $h_m(x)$ to pseudo-residuals: $\\{(x_i, r_{im})\\}_{i=1}^n$.\n",
        "> > >\n",
        "> > > c. Compute multiplier $\\gamma_m$:\n",
        "> > >\n",
        "> > > $\\gamma_m = \\underset{\\gamma}{\\operatorname{arg\\,min}} \\sum_{i=1}^n L\\left(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i)\\right)$.\n",
        "> > >\n",
        "> > > d. Update the model: $F_{m}(x)=F_{{m-1}}(x)+\\gamma _{m}h_{m}(x)$.\n",
        "> >\n",
        "> > 3. Output $F_M(x)$.\n",
        "\n",
        "\n",
        "## State-of-the-Art\n",
        "\n",
        "- LightGBM: A highly efficient gradient boosting decision tree\n",
        "\n",
        "### Gradient-based One-Side Sampling (GOSS):\n",
        "> Instances with larger gradients (i.e., under-trained instances) will contribute more to the information gain.\n",
        ">\n",
        "> âž” When down sampling the data instances, in order to retain the accuracy of information gain estimation, we should better keep those instances with large gradients, and only randomly drop those instances with small gradients.\n",
        "\n",
        "#### Algorithm Description\n",
        "> The gradient for each data instance in GBDT provides us with useful information for data sampling.\n",
        ">\n",
        "> âž” If an instance is associated with a small gradient, the training error for this instance is small and it is already well-trained.\n",
        ">\n",
        "> âž” discard those data instances with small gradients.\n",
        ">\n",
        "> However, the data distribution will be changed by doing so, which will hurt the accuracy of the learned model. ðŸ Š GOSS\n",
        ">\n",
        "> GOSS keeps all the instances with large gradients and performs random sampling on the instances with small gradients. In order to compensate the influence to the data distribution, when computing the information gain, GOSS introduces a constant multiplier for the data instances with small gradients.\n",
        "\n",
        "> ![algo2](https://drive.google.com/uc?id=1PfZSMs65hz-XdWsHAR5H4e2cZYOWY0ui)\n",
        ">\n",
        "> 1. sorts the data instances according to the absolute value of their gradients\n",
        "> 2. selects the top aÃ—100% instances\n",
        "> 3. it randomly samples bÃ—100% instances from the rest of the data.\n",
        "> 4. amplifies the sampled data with small gradients by a constant (1âˆ’a)/b when calculating the information gain.\n",
        "\n",
        "### Theoretical Analysis\n",
        "\n",
        "> $ \\{{g1, Â· Â· Â· , gn\\}} $  : the negative gradients of the loss function with respect to the output of the model\n",
        ">\n",
        "> The decision tree model splits each node at the most informative feature (with the largest information gain).\n",
        ">\n",
        "> âž” For **GBDT**, the information gain is usually measured by the variance after splitting:\n",
        "\n",
        "> Definition $3.1$ Let $O$ be the training dataset on a fixed node of the decision tree. The variance gain of splitting feature $j$ at point $d$ for this node is defined as:\n",
        ">\n",
        "> $$\n",
        "\\begin{gathered}\n",
        "V_{j \\mid O}(d)=\\frac{1}{n_O}\\left(\\frac{\\left(\\sum_{\\left\\{x_i \\in O: x_{i j} \\leq d\\right\\}} g_i\\right)^2}{n_{l \\mid O}^j(d)}+\\frac{\\left(\\sum_{\\left\\{x_i \\in O: x_{i j}>d\\right\\}} g_i\\right)^2}{n_{r \\mid O}^j(d)}\\right) \\\\\n",
        "\\text { where } n_O=\\sum I\\left[x_i \\in O\\right], n_{l \\mid O}^j(d)=\\sum I\\left[x_i \\in O: x_{i j} \\leq d\\right] \\text { and } n_{r \\mid O}^j(d)=\\sum I\\left[x_i \\in O: x_{i j}>d\\right] .\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "> For feature $j$, the decision tree algorithm selects $d^*_j = {argmax}_{d} V_j(d)$ and calculates the largest gain $V_j(d^*_j)$. Then, the data are split according to feature $j^*$ at point $d_{j^*}$ into the left and right child nodes.\n",
        "\n",
        "> A : keep the top-a Ã— 100% instances with the larger gradients\n",
        ">\n",
        "> B : A^c consisting (1 âˆ’ a) Ã— 100% instances with smaller gradients\n",
        ">\n",
        "> $$\n",
        "\\tilde{V}_j(d)=\\frac{1}{n}\\left(\\frac{\\left(\\sum_{x_i \\in A_l} g_i+\\frac{1-a}{b} \\sum_{x_i \\in B_l} g_i\\right)^2}{n_l^j(d)}+\\frac{\\left(\\sum_{x_i \\in A_r} g_i+\\frac{1-a}{b} \\sum_{x_i \\in B_r} g_i\\right)^2}{n_r^j(d)}\\right)\n",
        "$$\n",
        ">\n",
        "> where $A_l=\\left\\{x_i \\in A: x_{i j} \\leq d\\right\\}, A_r=\\left\\{x_i \\in A: x_{i j}>d\\right\\}, B_l=\\left\\{x_i \\in B: x_{i j} \\leq d\\right\\}, B_r=\\left\\{x_i \\in B\\right. : \\left.x_{i j}>d\\right\\}$, and the coefficient $\\frac{1-a}{b}$ is used to normalize the sum of the gradients over $B$ back to the size of $A^c$.\n",
        "\n",
        "> **GOSS** will not lose much training accuracy and will outperform random sampling\n",
        "\n",
        "> Theorem $3.2$ We denote the approximation error in GOSS as $\\mathcal{E}(d)=\\left|\\tilde{V}_j(d)-V_j(d)\\right|$ and $\\bar{g}_l^j(d)=\n",
        "\\frac{\\sum_{x_i \\in\\left(A \\cup A^c\\right)_l\\left|g_i\\right|}}{n_l^j(d)}, \\bar{g}_r^j(d)=\\frac{\\sum_{x_i \\in\\left(A \\cup A^c\\right)_r\\left|g_i\\right|}}{n_r^j(d)}$. With probability at least $1 âˆ’ Î´$, we have\n",
        ">\n",
        "> $$\n",
        "\\mathcal{E}(d) \\leq C_{a, b}^2 \\ln 1 / \\delta \\cdot \\max \\left\\{\\frac{1}{n_l^j(d)}, \\frac{1}{n_r^j(d)}\\right\\}+2 D C_{a, b} \\sqrt{\\frac{\\ln 1 / \\delta}{n}},\n",
        "$$\n",
        ">\n",
        ">where $C_{a, b}=\\frac{1-a}{\\sqrt{b}} \\max _{x_i \\in A^c}\\left|g_i\\right|$, and $D=\\max \\left(\\bar{g}_l^j(d), \\bar{g}_r^j(d)\\right)$.\n",
        "\n",
        "> The generalization error with GOSS will be close to that calculated by using the full data instances if the GOSS approximation is accurate. On the other hand, sampling will increase the diversity of the base learners, which potentially help to improve the generalization performance.\n",
        "(The larger n, and the more evenly the instances are split into the left and right leaf through the split point, the smallest the approximation error becomes.)\n",
        "\n",
        "### Exclusive Feature Bundling (EFB):\n",
        "\n",
        "> High-dimensional data are usually very sparse.\n",
        ">\n",
        "> Designing a nearly lossless approach to reduce the number of features. Specifically, in a sparse feature space, many features are mutually exclusive, i.e., they never take nonzero values simultaneously.\n",
        "\n",
        "> Theorem $4.1$ The problem of partitioning features into a smallest number of exclusive bundles is NP-hard.\n",
        ">\n",
        "> Proof: We will reduce the graph coloring problem to our problem. Since graph coloring problem is NP-hard, we can then deduce our conclusion.\n",
        ">\n",
        "> we first reduce the optimal bundling problem to the graph coloring problem by taking features as vertices and adding edges for every two features if they are not mutually exclusive, then we use a greedy algorithm which can produce reasonably good results for graph coloring to produce the bundles.\n",
        ">\n",
        "> There are usually quite a few features, although not 100% mutually exclusive, also rarely take nonzero values simultaneously.\n",
        ">\n",
        "> If our algorithm can allow a small fraction of conflicts, we can get an even smaller number of feature bundles and further improve the computational efficiency.\n",
        "\n",
        "> ![algo2](https://drive.google.com/uc?id=1EuaqQGrG0OFtD3bU615YPd6yOoAlceGu)\n",
        "\n",
        "\n",
        "> 3-1. Construct a graph with weighted edges, whose weights correspond to the total conflicts between features.\n",
        ">\n",
        "> 3-2. Sort the features by their degrees in the graph in the descending order\n",
        ">\n",
        "> 3-3. Check each feature in the ordered list, and either assign it to an existing bundle with a small conflict, or create a new bundle.\n",
        "\n",
        "\n",
        "> 4-1. The values of the original features can be identified from the feature bundles.\n",
        ">\n",
        "> 4-2. We can construct a feature bundle by letting exclusive features reside in different bins. This can be done by adding offsets to the original values of the features.\n",
        "\n",
        "\n",
        "# Dataset\n",
        "\n",
        "> [Apple - 10 Year Stock Price History](https://www.kaggle.com/datasets/aleksandrdubrovin/apple-stock-price-history) , [Source](https://www.investing.com/equities/apple-computer-inc-historical-data)\n",
        ">\n",
        "> The dataset \"Apple - 10 Year Stock Price History\" contains the historical stock prices of Apple Inc (**AAPL**) traded on **NASDAQ**. The data covers a period of **10 years** and includes *columns such as date, open, high, low, close and volume of stock traded*. The data is available on **Kaggle** and can be used for various purposes such as time-series analysis, stock price prediction and other **financial** analysis.\n",
        ">\n",
        "> rows 2539 * 7 coulmns\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r7KDa_TuoqW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Implementation"
      ],
      "metadata": {
        "id": "RcJ8XFs3bh1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import joblib\n",
        "import random as rn\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from lightgbm import LGBMRegressor\n",
        "import os\n",
        "\n"
      ],
      "metadata": {
        "id": "NjLNUxDbbhAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def time_transform(df):\n",
        "    date = datetime.strptime(df,'%b %d, %Y')\n",
        "    return date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "def make_input(dataset, time_step=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-time_step-1):\n",
        "        a = dataset[i:(i+time_step), 0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + time_step, 0])\n",
        "    return np.array(dataX), np.array(dataY)"
      ],
      "metadata": {
        "id": "Ap-XMfZzk3Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reproducibility\n",
        "seed_num = 42\n",
        "np.random.seed(seed_num)\n",
        "rn.seed(seed_num)\n",
        "os.environ['PYTHONHASHSEED']=str(seed_num)"
      ],
      "metadata": {
        "id": "vCIodutqbrYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load and visualize dataset"
      ],
      "metadata": {
        "id": "huXY0y0VbvCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/AAPL Historical Data.csv\")"
      ],
      "metadata": {
        "id": "9bbxM14AbuWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Date'] = df['Date'].apply(lambda x: time_transform(x))\n",
        "dataset = df.sort_values('Date').reset_index(drop=True)"
      ],
      "metadata": {
        "id": "0aRgXQZ7bx5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of dataset :\", dataset.shape)\n",
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "d73_y5nUbzLj",
        "outputId": "cae9efbb-0874-4ee9-ef1a-159d59ea4c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dataset : (2539, 7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Date  Price   Open   High    Low     Vol. Change %\n",
              "0  2012-02-27  18.78  18.62  18.87  18.44  547.58M    0.64%\n",
              "1  2012-02-28  19.12  18.86  19.12  18.78  600.39M    1.81%\n",
              "2  2012-02-29  19.37  19.34  19.56  19.13  952.00M    1.31%\n",
              "3  2012-03-01  19.45  19.58  19.58  19.24  683.25M    0.41%\n",
              "4  2012-03-02  19.47  19.44  19.53  19.38  431.71M    0.10%"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8b812d7a-81fd-4942-a0c8-111a416fdef7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Price</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Vol.</th>\n",
              "      <th>Change %</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2012-02-27</td>\n",
              "      <td>18.78</td>\n",
              "      <td>18.62</td>\n",
              "      <td>18.87</td>\n",
              "      <td>18.44</td>\n",
              "      <td>547.58M</td>\n",
              "      <td>0.64%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2012-02-28</td>\n",
              "      <td>19.12</td>\n",
              "      <td>18.86</td>\n",
              "      <td>19.12</td>\n",
              "      <td>18.78</td>\n",
              "      <td>600.39M</td>\n",
              "      <td>1.81%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012-02-29</td>\n",
              "      <td>19.37</td>\n",
              "      <td>19.34</td>\n",
              "      <td>19.56</td>\n",
              "      <td>19.13</td>\n",
              "      <td>952.00M</td>\n",
              "      <td>1.31%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2012-03-01</td>\n",
              "      <td>19.45</td>\n",
              "      <td>19.58</td>\n",
              "      <td>19.58</td>\n",
              "      <td>19.24</td>\n",
              "      <td>683.25M</td>\n",
              "      <td>0.41%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2012-03-02</td>\n",
              "      <td>19.47</td>\n",
              "      <td>19.44</td>\n",
              "      <td>19.53</td>\n",
              "      <td>19.38</td>\n",
              "      <td>431.71M</td>\n",
              "      <td>0.10%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b812d7a-81fd-4942-a0c8-111a416fdef7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8b812d7a-81fd-4942-a0c8-111a416fdef7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8b812d7a-81fd-4942-a0c8-111a416fdef7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "date = dataset['Date'].values\n",
        "close = dataset['Price'].values"
      ],
      "metadata": {
        "id": "hVEitfVJb0uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocessing"
      ],
      "metadata": {
        "id": "Z9XxfWbQb2ON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "scaled_close = scaler.fit_transform(np.array(close).reshape(-1,1))"
      ],
      "metadata": {
        "id": "iiZhm8Sqb23H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(len(close)*0.795)\n",
        "val_size = len(close)-train_size\n",
        "# train_data, val_data = scaled_close[0:train_size], scaled_close[train_size:len(close)]\n",
        "\n",
        "train_data, val_data = close[0:train_size], close[train_size:len(close)]\n",
        "train_data = np.reshape(train_data, (train_data.shape[0], 1))\n",
        "val_data = np.reshape(val_data, (val_data.shape[0], 1))"
      ],
      "metadata": {
        "id": "MRx0phw2b5lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_step = 1\n",
        "X_train, y_train = make_input(train_data, time_step)\n",
        "X_val, y_val = make_input(val_data, time_step)"
      ],
      "metadata": {
        "id": "QvyeKFVob605"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X train shape :\", X_train.shape)\n",
        "print(\"y train shape :\", y_train.shape)\n",
        "print(\"X val shape :\", X_val.shape)\n",
        "print(\"y val shape :\", y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjsTPUcrb8JH",
        "outputId": "3d592886-3d0a-4133-8b6c-44b93f501831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X train shape : (2016, 1)\n",
            "y train shape : (2016,)\n",
            "X val shape : (519, 1)\n",
            "y val shape : (519,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Modeling - LightGBM"
      ],
      "metadata": {
        "id": "8wt6Z9STcAtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "        'n_estimators': 10000,\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'max_depth': -1,\n",
        "        'learning_rate': 0.01,\n",
        "        'subsample': 0.72,\n",
        "        'subsample_freq': 4,\n",
        "        'feature_fraction': 0.4,\n",
        "        'lambda_l1': 1,\n",
        "        'lambda_l2': 1,\n",
        "        'seed': seed_num,\n",
        "        }"
      ],
      "metadata": {
        "id": "2lIIkBgsRng3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LGBMRegressor(**params)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwZ4H6dEcC54",
        "outputId": "99f68b18-09b9-4110-8c97-df7c75b8f1a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LGBMRegressor(feature_fraction=0.4, lambda_l1=1, lambda_l2=1,\n",
              "              learning_rate=0.01, metric='rmse', n_estimators=10000,\n",
              "              objective='regression', seed=42, subsample=0.72,\n",
              "              subsample_freq=4)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Evaluation"
      ],
      "metadata": {
        "id": "jIN-tHthcGHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_predicton = model.predict(X_train)\n",
        "val_prediction = model.predict(X_val)\n",
        "\n",
        "print(\"Train pred shape :\", train_predicton.shape)\n",
        "print(\"Val pred shape :\", val_prediction.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoC9dF7ocGtH",
        "outputId": "d4e7ef62-e575-43d5-eaa3-64ddfdeba4e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train pred shape : (2016,)\n",
            "Val pred shape : (519,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_pred = scaler.inverse_transform(train_predicton.reshape(-1,1))\n",
        "# val_pred = scaler.inverse_transform(val_predicton.reshape(-1,1))\n",
        "# print(val_pred[:5])"
      ],
      "metadata": {
        "id": "rH2v1ZoZcJnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_prediction[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4JyBO4kTU8c",
        "outputId": "a8668ad1-8c4c-4768-a0c7-73bda8cf42e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[75.36302961 73.0791612  73.0791612  66.53690579 71.02891821]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "close = np.array(close)\n",
        "cc = close.reshape((len(close), 1))\n",
        "tt1, tt2 = cc[0:train_size,:], cc[train_size:len(close),:]\n",
        "xtrain, ytrain = make_input(tt1, 1)\n",
        "Xval, yval = make_input(tt2, 1)"
      ],
      "metadata": {
        "id": "s82rwZ9NcKpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_rmse = math.sqrt(mean_squared_error(yval, val_prediction))\n",
        "print('Val RMSE: %.3f' % val_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx_59WnlcMFn",
        "outputId": "6d171921-e020-49fe-ecaf-eb9cd7ef141f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 57.708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Change Parameters"
      ],
      "metadata": {
        "id": "FdOPoU42Mbi0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**'n_estimators'**: This parameter determines the **number of trees** in the model. If we have a large dataset, we may increase the value of this parameter. If the model is overfitting, we may decrease the value.\n",
        "\n",
        "**'learning_rate'**: The learning_rate controls the magnitude of updates to the model **weights** during training. If the model is underfitting, we can increase the value of this parameter to allow the model to make larger updates to the weights. If the model is overfitting, we can decrease the value to reduce the magnitude of the updates to the weights. We can try reducing the learning_rate to a smaller value, such as 0.001 or 0.0001, to see if it results in better performance.\n",
        "\n",
        "**'max_depth'**: We can consider increasing the max_depth from -1 to a higher value. This will allow our model to learn **more complex relationships** in the data.\n",
        "\n",
        "\n",
        "**'feature_fraction'**: This parameter controls the **fraction of features** to be considered when training the model. We can try increasing the feature_fraction to 0.8 or higher to see if it results in better performance."
      ],
      "metadata": {
        "id": "Nzymchq2Jov4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'n_estimators': 10000 to 20000\n",
        "# 'learning_rate': 0.001 to 0.0005\n",
        "\n",
        "params2 = {\n",
        "        'n_estimators': 20000,\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'max_depth': -1,\n",
        "        'learning_rate': 0.0005,\n",
        "        'subsample': 0.72,\n",
        "        'subsample_freq': 4,\n",
        "        'feature_fraction': 0.4,\n",
        "        'lambda_l1': 1,\n",
        "        'lambda_l2': 1,\n",
        "        'seed': seed_num,\n",
        "        }"
      ],
      "metadata": {
        "id": "B4ppRLRz_bhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LGBMRegressor(**params2)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "train_predicton = model.predict(X_train)\n",
        "val_prediction = model.predict(X_val)\n",
        "\n",
        "print(\"Train pred shape :\", train_predicton.shape)\n",
        "print(\"Val pred shape :\", val_prediction.shape)\n",
        "\n",
        "\n",
        "print(val_prediction[:5])\n",
        "\n",
        "\n",
        "close = np.array(close)\n",
        "cc = close.reshape((len(close), 1))\n",
        "tt1, tt2 = cc[0:train_size,:], cc[train_size:len(close),:]\n",
        "xtrain, ytrain = make_input(tt1, 1)\n",
        "Xval, yval = make_input(tt2, 1)\n",
        "\n",
        "\n",
        "val_rmse2 = math.sqrt(mean_squared_error(yval, val_prediction))\n",
        "print('Val RMSE: %.3f' % val_rmse2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3MuH2DcBdOn",
        "outputId": "e7dd27d2-444b-4d69-d164-fc86578f9f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train pred shape : (2016,)\n",
            "Val pred shape : (519,)\n",
            "[75.56194682 72.81977439 72.81977439 66.55446301 71.12832161]\n",
            "Val RMSE: 57.698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosting"
      ],
      "metadata": {
        "id": "5r-e3RFSmBYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "gbr = GradientBoostingRegressor()"
      ],
      "metadata": {
        "id": "f9NsbyZal0m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gbr.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FEQz885l5hv",
        "outputId": "429b48d0-c8a1-49cb-a707-82ef2395f750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor()"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_predicton = gbr.predict(X_train)\n",
        "val_prediction = gbr.predict(X_val)\n",
        "\n",
        "print(\"Train pred shape :\", train_predicton.shape)\n",
        "print(\"Val pred shape :\", val_prediction.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92acdarLnUiC",
        "outputId": "8d6b1cbc-9248-4b82-d131-154f82074c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train pred shape : (2016,)\n",
            "Val pred shape : (519,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_prediction[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_ihqO08naqZ",
        "outputId": "78202e04-cf90-4c28-a4a3-9d8406331fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[77.45586171 70.58248212 72.45257234 66.6480217  72.23313364]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GB_val_rmse = math.sqrt(mean_squared_error(yval, val_prediction))\n",
        "print('Val RMSE: %.3f' % GB_val_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVEaPZC8nnGx",
        "outputId": "deee0ef0-c2ab-4095-a004-9b383bb4bfd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 105.014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X5p-FZh7i7s4"
      }
    }
  ]
}